{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da19e4d4",
   "metadata": {},
   "source": [
    "# Регрессионная модель изменения цен на дома в Бостоне\n",
    "\n",
    "# Выполнила Матюшина Алина БСТ1802\n",
    "\n",
    "\n",
    "# Цель\n",
    "Реализовать предсказание медианной цены на дома в пригороде Бостона в середине 1970-х по таким данным, как уровень преступности, ставка местного имущественного налога и т.д.\n",
    "Данный набор содержит относительно немного образцов данных: всего 506, разбитых на 404 обучающих и 102 контрольных образца. И каждый признак во входных данных(например, уровень преступности) имеет свой масштаб. Например, некоторые признаки\n",
    "являются пропорциями и имеют значения между 0 и 1, другие — между 1 и 12 и т. д.\n",
    "Не путайте регрессию с алгоритмом логистической регрессии. Как ни странно, логистическая регрессия не является регрессионным алгоритмом — это алгоритм классификации.\n",
    "\n",
    "# Задачи\n",
    "\n",
    "* Ознакомиться с задачей регрессии\n",
    "* Изучить отличие задачи регрессии от задачи классификации\n",
    "* Создать модель\n",
    "* Настроить параметры обучения\n",
    "* Обучить и оценить модели\n",
    "* Ознакомиться с перекрестной проверкой\n",
    "\n",
    "# Выполнение работы\n",
    "\n",
    "\n",
    "### Подключение модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe768b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c91af",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "Загрузим данные по поставленным целям и выведим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3ebfc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2 18.6 14.5 17.8 50.\n",
      " 20.8 24.3 24.2 19.8 19.1 22.7 12.  10.2 20.  18.5 20.9 23.  27.5 30.1\n",
      "  9.5 22.  21.2 14.1 33.1 23.4 20.1  7.4 15.4 23.8 20.1 24.5 33.  28.4\n",
      " 14.1 46.7 32.5 29.6 28.4 19.8 20.2 25.  35.4 20.3  9.7 14.5 34.9 26.6\n",
      "  7.2 50.  32.4 21.6 29.8 13.1 27.5 21.2 23.1 21.9 13.  23.2  8.1  5.6\n",
      " 21.7 29.6 19.6  7.  26.4 18.9 20.9 28.1 35.4 10.2 24.3 43.1 17.6 15.4\n",
      " 16.2 27.1 21.4 21.5 22.4 25.  16.6 18.6 22.  42.8 35.1 21.5 36.  21.9\n",
      " 24.1 50.  26.7 25. ]\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5571bb6",
   "metadata": {},
   "source": [
    "Проведем нормализацию, для облегчения обучения нейроной сети. \n",
    "Для каждого признака во входных данных (столбца в матрице входных данных) из каждого значения вычитается среднее по этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение, равное единице. Такую нормализацию легко выполнить с помощью Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567a4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим среднее значение(сз) для каждого столбца \n",
    "mean = train_data.mean(axis=0)\n",
    "# Вычисляем отклонение от сз для каждого столбца\n",
    "train_data -= mean\n",
    "# Вычисляем стандартное отклонение для каждого столбца\n",
    "std = train_data.std(axis=0)\n",
    "# Централизация данных\n",
    "train_data /= std \n",
    "\n",
    "# Централизация и нормирование тестовых данных\n",
    "test_data -= mean #\n",
    "test_data /= std #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04dbe0",
   "metadata": {},
   "source": [
    "Определим функцию build_model() в которой:\n",
    "* Определим последовательность модели нейронной сети (от меньшего к большему)\n",
    "* Добавление слоя модели с использованием функции активации ReLu (возвращает значение х, если х положительно, иначе 0)\n",
    "* Добавления слоя с предсказание \"цены\" в любом деапозоне\n",
    "* Добавление параметра обучения:\n",
    "    1. Оптимизация - Сохранение скользящее среднее значение квадрата градиентов и резделение градиента на корень из этого среднего значения\n",
    "    2. Потери - Средняя квадратическая ошибка\n",
    "    3. Метрика для мониторинга на этапах обучения и тестирования - средняя абсолютная ошибка (абсолютное значение разности между целевым и вычисленным значениями)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a331a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # input_shape - указание на то что, на вход идет 13 параметров \n",
    "    model.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc369996",
   "metadata": {},
   "source": [
    "Хорошей практикой в таких ситуациях является применение перекрестной проверки по K\n",
    "блокам (K-fold cross-validation). Суть ее заключается в разделении доступных данных на K\n",
    "блоков (обычно K = 4 или 5), создании K идентичных моделей и обучении каждой на K—1\n",
    "блоках с оценкой по оставшимся блокам. По полученным K оценкам вычисляется среднее\n",
    "значение, которое принимается как оценка модели. В коде такая проверка реализуется\n",
    "достаточно просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12940f60",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:71\u001b[1;36m\u001b[0m\n\u001b[1;33m    ``дратичная ошибка по всем моделям в ходе обучения\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def elsTask(k, num_epochs):\n",
    "    num_val_samples = len(train_data) // k # строк в каждом блоке\n",
    "    all_scores = [] # Список результатов работ \n",
    "\n",
    "    acc = []\n",
    "    loss = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        #Выбор данных для тестовой нейроной сети\n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        #Выбор данных для обучения нейроной сеити\n",
    "        partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]],axis=0)\n",
    "        partial_train_targets = np.concatenate([train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "        # Создание модели нейросети\n",
    "        model = build_model()\n",
    "\n",
    "        # Адаптации модели под обучающие данные\n",
    "        d = model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "        #Оценка модели\n",
    "        #Вычисляет значение потерь и значения всех метрик, которые мы выбрали при составлении модели.\n",
    "        val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
    "\n",
    "        #Добавление результатов работы в список \n",
    "        all_scores.append(val_mae)\n",
    "\n",
    "\n",
    "        acc.append(d.history[\"mae\"])\n",
    "        loss.append(d.history[\"loss\"])\n",
    "\n",
    "        \n",
    "    print (np.mean(all_scores))\n",
    "    \n",
    "    #Построим графики Абсолютное отклонение по всем моделям в ходе обучения\n",
    "    x = [x for x in range(num_epochs)]\n",
    "    \n",
    "    if k == 1: \n",
    "        plt.plot(x, acc[0], 'red')\n",
    "    elif k == 2: \n",
    "        plt.plot(x, acc[0], 'red', x, acc[1], 'green')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model'])\n",
    "    elif k == 3: \n",
    "        plt.plot(x, acc[0], 'red', x, acc[1], 'green', x, acc[2], 'blue')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model'])\n",
    "    elif k == 4: \n",
    "        plt.plot(x, acc[0], 'red', x, acc[1], 'green', x, acc[2], 'blue', x, acc[3], 'black')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model','black - fourth model'])\n",
    "    else: \n",
    "        plt.plot(x, acc[0], 'red', x, acc[1], 'green', x, acc[2], 'blue', x, acc[3], 'black', x, acc[3], 'pink')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model','black - fourth model', 'pink - fifth model'])\n",
    "    \n",
    "    plt.ylabel(\"Абсолютная ошибка\")\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.title(\"Абсолютное отклонение по всем моделям в ходе обучения\")\n",
    "    plt.show()\n",
    "\n",
    "    #Усредненное абсолютное отклонение\n",
    "    y = []\n",
    "    for i in range(num_epochs):\n",
    "        y.append(np.mean([acc[j][i] for j in range(k)]))\n",
    "\n",
    "    plt.plot(x, y, 'red')\n",
    "    plt.ylabel(\"Абсолютная ошибка (среднее)\")\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.title(\"Усредненное абсолютное отклонение\")\n",
    "    plt.show()\n",
    "\n",
    "    #Среднеквадратичная ошибка по всем моделям в ходе обучения\n",
    "    if k == 1: \n",
    "        plt.plot(x, loss[0], 'red')\n",
    "    elif k == 2: \n",
    "        plt.plot(x, loss[0], 'red', x, loss[1], 'green')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model'])\n",
    "    elif k == 3: \n",
    "        plt.plot(x, loss[0], 'red', x, loss[1], 'green', x, loss[2], 'blue')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model'])\n",
    "    elif k == 4: \n",
    "        plt.plot(x, loss[0], 'red', x, loss[1], 'green', x, loss[2], 'blue', x, loss[3], 'black')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model','black - fourth model'])\n",
    "    else: \n",
    "        plt.plot(x, loss[0], 'red', x, loss[1], 'green', x, loss[2], 'blue', x, loss[3], 'black', x, loss[3], 'pink')\n",
    "        plt.legend(['red - first model','green - second model','blue - third model','black - fourth model', 'pink - fifth model'])\n",
    "    plt.ylabel(\"Среднеквадратичная ошибка (отклонение)\")\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.title(\"Среднеквадратичная ошибка по всем моделям в ходе обучения\")\n",
    "    plt.show()\n",
    "\n",
    "    #Усредненная среднеквадратичная ошибка\n",
    "    y = []\n",
    "    for i in range(num_epochs):\n",
    "        y.append(np.mean([loss[j][i] for j in range(k)]))\n",
    "\n",
    "    plt.plot(x, y, 'red', )\n",
    "    plt.ylabel(\"Среднеквадратичная ошибка (отклонение) (среднее)\")\n",
    "    plt.xlabel(\"Эпоха\")\n",
    "    plt.title(\"Средненная среднеквадратичная ошибка\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4352f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elsTask(1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c960a02",
   "metadata": {},
   "source": [
    "Проведем повторное моделирование, но с увеличением кол-во эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elsTask(4,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd2629",
   "metadata": {},
   "source": [
    "Увеличем количество блоков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "elsTask(6,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ada83",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "Самой точно моделью оказалась та, которая имеет больше блоков, при одинаковом количестве эпох. При увлечение эпох в 1.5 раза, изменилось абсолютное отклонение в лучшую сторону, но не кретически. При увеличение количество блоков на один, так же привело к снижению абсолютного отклонения. Таким образом, нужно стремится к \"балансу\" между колличеством эпох и колличеством блоков, для того чтобы прийти к оптимальным значениям и неперегружать систему.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
